# python
"""
planner_agent.py - PlannerAgent æ ¸å¿ƒå®žçŽ°ï¼ˆä¿ç•™åŽŸæœ‰æ•°æ®ç»“æž„ï¼‰
===========================================================

åŠŸèƒ½ï¼šæœ¬å­¦æœŸè¯¾ç¨‹è§„åˆ’å’Œå¤ä¹ è®¡åˆ’ç”Ÿæˆï¼ˆåŸºäºŽ RAG + LLM æž¶æž„ï¼‰

æ ¸å¿ƒç‰¹ç‚¹ï¼š
1. å®Œæ•´çš„ RAG æž¶æž„å®žçŽ°ï¼ˆæ£€ç´¢ + å¢žå¼º + ç”Ÿæˆï¼‰
2. LLM é…ç½®çµæ´»ï¼Œæ”¯æŒæœ¬åœ°æ¨¡åž‹ï¼ˆOllama, vLLM, è‡ªå®šä¹‰ APIï¼‰
3. ä¸“æ³¨äºŽæœ¬å­¦æœŸè§„åˆ’ï¼ˆ9-12æœˆï¼‰
4. æ‰€æœ‰å‚æ•°å¯åŽæœŸä¿®æ”¹ï¼Œæ— éœ€å†™æ­»
5. ä¿ç•™åŽŸæœ‰çš„æ•°æ®ç»“æž„ï¼Œä¸åšæ›´æ”¹

æ•°æ®ç»“æž„ç”± init.py å®šä¹‰ï¼Œè¿™é‡Œåªå®žçŽ° Agent çš„æ–¹æ³•
"""

from datetime import datetime
from typing import Dict, List
import requests
import json

from dao import (
    UserCourseDAO,
    CourseDAO, AssignmentDAO,
    StudySessionDAO
)
from models import ActionInput, ActionResult, ScheduleResult, ScheduleInput


class LLMConfig:
    """LLM é…ç½®ç±» - æ”¯æŒå¤šç§æ¨¡åž‹å’Œæä¾›å•†"""

    def __init__(
            self,
            provider: str = "openai",
            base_url: str = None,
            api_key: str = None,
            model: str = "gpt-4",
            temperature: float = 0.7,
            max_tokens: int = 2500,
            timeout: int = 30
    ):
        self.provider = provider
        self.base_url = base_url
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout

    @staticmethod
    def openai_api(api_key: str, model: str = "gpt-4") -> "LLMConfig":
        return LLMConfig(provider="openai", api_key=api_key, model=model, base_url="https://api.openai.com/v1")

    @staticmethod
    def local_ollama(base_url: str = "http://localhost:11434", model: str = "qwen2:7b") -> "LLMConfig":
        return LLMConfig(provider="ollama", base_url=base_url, model=model)

    @staticmethod
    def local_vllm(base_url: str = "http://localhost:8000", model: str = "meta-llama/Llama-2-7b-hf") -> "LLMConfig":
        return LLMConfig(provider="vllm", base_url=base_url, model=model)

    @staticmethod
    def local_api(base_url: str, model: str = "local-model") -> "LLMConfig":
        return LLMConfig(provider="local_api", base_url=base_url, model=model)

    def to_dict(self) -> Dict:
        return {
            'provider': self.provider,
            'base_url': self.base_url,
            'api_key': self.api_key if self.api_key and len(self.api_key) > 10 else '***',
            'model': self.model,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'timeout': self.timeout
        }


class PlannerAgent:
    """å­¦æœŸè§„åˆ’æ™ºèƒ½ä½“ï¼ˆRAG + LLM + é…ç½®åŒ– + æœ¬å­¦æœŸä¸“æ³¨ï¼‰"""

    def __init__(
            self,
            llm_config: LLMConfig = None,
            openai_api_key: str = None,
            current_semester: str = "2025-Fall"
    ):
        self.current_semester = current_semester
        self.semester_months = [9, 10, 11, 12]

        # åˆå§‹åŒ– DAO
        self.user_course_dao = UserCourseDAO()
        self.course_dao = CourseDAO()
        self.assignment_dao = AssignmentDAO()
        self.study_session_dao = StudySessionDAO()

        # é…ç½® LLM
        if llm_config:
            self.llm_config = llm_config
        elif openai_api_key:
            self.llm_config = LLMConfig.openai_api(openai_api_key)
        else:
            self.llm_config = LLMConfig.local_ollama()

    # ================================================================
    # LLM è°ƒç”¨æ–¹æ³•ï¼ˆæ”¯æŒå¤šä¸ªæä¾›å•†ï¼‰
    # ================================================================

    def call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """è°ƒç”¨ LLM APIï¼ˆæ”¯æŒå¤šä¸ªæä¾›å•†ï¼‰"""
        try:
            if self.llm_config.provider == "openai":
                return self._call_openai(system_prompt, user_prompt)
            elif self.llm_config.provider == "ollama":
                return self._call_ollama(system_prompt, user_prompt)
            elif self.llm_config.provider == "vllm":
                return self._call_vllm(system_prompt, user_prompt)
            elif self.llm_config.provider == "local_api":
                return self._call_local_api(system_prompt, user_prompt)
            else:
                return f"é”™è¯¯ï¼šä¸æ”¯æŒçš„æä¾›å•† {self.llm_config.provider}"
        except Exception as e:
            print(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            return f"é”™è¯¯ï¼šæ— æ³•ç”Ÿæˆå›žç­”ã€‚{str(e)}"

    def _call_openai(self, system_prompt: str, user_prompt: str) -> str:
        """è°ƒç”¨ OpenAI API"""
        import openai
        openai.api_key = self.llm_config.api_key
        response = openai.ChatCompletion.create(
            model=self.llm_config.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=self.llm_config.temperature,
            max_tokens=self.llm_config.max_tokens
        )
        return response['choices'][0]['message']['content']

    def _call_ollama(self, system_prompt: str, user_prompt: str) -> str:
        """è°ƒç”¨æœ¬åœ° Ollama"""
        url = f"{self.llm_config.base_url}/api/chat"
        payload = {
            "model": self.llm_config.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": self.llm_config.temperature,
            "stream": False
        }
        response = requests.post(url, json=payload, timeout=self.llm_config.timeout)
        if response.status_code == 200:
            return response.json()['message']['content']
        else:
            raise Exception(f"Ollama API é”™è¯¯: {response.status_code}")

    def _call_vllm(self, system_prompt: str, user_prompt: str) -> str:
        """è°ƒç”¨æœ¬åœ° vLLM"""
        url = f"{self.llm_config.base_url}/v1/chat/completions"
        payload = {
            "model": self.llm_config.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": self.llm_config.temperature,
            "max_tokens": self.llm_config.max_tokens
        }
        response = requests.post(url, json=payload, timeout=self.llm_config.timeout)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            raise Exception(f"vLLM API é”™è¯¯: {response.status_code}")

    def _call_local_api(self, system_prompt: str, user_prompt: str) -> str:
        """è°ƒç”¨æœ¬åœ°é€šç”¨ API"""
        url = f"{self.llm_config.base_url}/chat"
        payload = {
            "model": self.llm_config.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": self.llm_config.temperature,
            "max_tokens": self.llm_config.max_tokens
        }
        response = requests.post(url, json=payload, timeout=self.llm_config.timeout)
        if response.status_code == 200:
            result = response.json()
            if 'content' in result:
                return result['content']
            elif 'message' in result:
                return result['message']
            else:
                return str(result)
        else:
            raise Exception(f"æœ¬åœ° API é”™è¯¯: {response.status_code}")

    # ================================================================
    # é…ç½®ç®¡ç†æ–¹æ³•
    # ================================================================

    def update_llm_config(self, **kwargs) -> None:
        """åŠ¨æ€æ›´æ–° LLM é…ç½®ï¼ˆåŽæœŸå¯ä¿®æ”¹ï¼Œæ— éœ€é‡æ–°åˆå§‹åŒ–ï¼‰"""
        for key, value in kwargs.items():
            if hasattr(self.llm_config, key):
                setattr(self.llm_config, key, value)

    def get_llm_config(self) -> Dict:
        """èŽ·å–å½“å‰ LLM é…ç½®"""
        return self.llm_config.to_dict()

    #
    # def chat(self, user_id: int, message: str) -> str:
    #
    # # ================================================================
    # # æ–°å¢žæ–¹æ³•ï¼šsuggest() - æä¾›å­¦ä¹ å»ºè®®
    # # ================================================================
    #
    # def suggest(self, task_desc: str, resources: List[str] = None, user_id: int = None) -> Dict:
    #
    #
    # # ================================================================
    # # æ–°å¢žæ–¹æ³•ï¼šschedule() - ç”Ÿæˆæ—¥ç¨‹å®‰æŽ’
    # # ================================================================
    #

    # python
    def schedule(self, scheduleInput: "ScheduleInput") -> "ScheduleResult":
        """
        æ™ºèƒ½æ—¥ç¨‹å®‰æŽ’Agent - ä½¿ç”¨é…ç½®çš„LLMè¿›è¡Œæ™ºèƒ½è§„åˆ’
        """
        try:
            user_id = getattr(scheduleInput, 'user_id', None)
            tasks = getattr(scheduleInput, 'tasks', [])
            date_range = getattr(scheduleInput, 'date_range', None)

            if not user_id:
                return ScheduleResult(schedule=[{
                    'type': 'error',
                    'content': 'éœ€è¦ç”¨æˆ·ID'
                }])

            print(f"ðŸ” å¼€å§‹æ™ºèƒ½æ—¥ç¨‹è§„åˆ’ - ç”¨æˆ·: {user_id}, ä»»åŠ¡æ•°: {len(tasks)}")
            print(f"ðŸ¤– ä½¿ç”¨æ¨¡åž‹: {self.llm_config.model}")

            # 1. èŽ·å–åŸºç¡€æ•°æ®
            relevant_data = self._get_relevant_data(user_id, date_range)

            # 2. ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½è§„åˆ’ï¼ˆå¦‚æžœLLMå¯ç”¨ï¼‰
            if self.llm_config.provider != "unknown":
                try:
                    ai_plan = self._generate_intelligent_plan(tasks, relevant_data, user_id)
                    return self._create_ai_schedule_response(ai_plan, relevant_data, tasks)
                except Exception as e:
                    print(f"âš ï¸ LLMè§„åˆ’å¤±è´¥ï¼Œä½¿ç”¨å›žé€€æ–¹æ¡ˆ: {e}")
                    return self._create_fallback_schedule(relevant_data, tasks)
            else:
                # LLMä¸å¯ç”¨ï¼Œä½¿ç”¨è§„åˆ™åŸºç¡€æ–¹æ¡ˆ
                return self._create_rule_based_schedule(relevant_data, tasks)

        except Exception as e:
            print(f"âŒ æ—¥ç¨‹è§„åˆ’é”™è¯¯: {e}")
            return ScheduleResult(schedule=[{
                'type': 'error',
                'content': f'è§„åˆ’å¤±è´¥: {str(e)}'
            }])

    def _get_relevant_data(self, user_id: int, date_range: str) -> Dict:
        """
        èŽ·å–ç›¸å…³æ•°æ®
        """
        from datetime import datetime, timedelta

        # è§£æžæ—¥æœŸèŒƒå›´
        if date_range and ' to ' in date_range:
            start_str, end_str = date_range.split(' to ')
            start_date = datetime.strptime(start_str.strip(), '%Y-%m-%d')
            end_date = datetime.strptime(end_str.strip(), '%Y-%m-%d')
        else:
            start_date = datetime.now()
            end_date = start_date + timedelta(days=30)

        assignments = self.assignment_dao.get_assignments_by_date_range(user_id, start_date, end_date)
        study_sessions = self.study_session_dao.get_study_sessions_by_date_range(user_id, start_date, end_date)

        return {
            'assignments': assignments,
            'study_sessions': study_sessions,
            'date_range': {'start': start_date, 'end': end_date}
        }

    def _generate_intelligent_plan(self, tasks: List[str], relevant_data: Dict, user_id: int) -> Dict:
        """ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½è§„åˆ’"""

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å­¦ä¹ è§„åˆ’åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·çš„ä»»åŠ¡å’ŒçŽ°æœ‰çš„å­¦ä¹ æ•°æ®ï¼Œç”Ÿæˆåˆç†çš„æ—¥ç¨‹å®‰æŽ’å»ºè®®ã€‚

    è¯·è€ƒè™‘ï¼š
    1. ä»»åŠ¡çš„ä¼˜å…ˆçº§å’Œå¤æ‚åº¦
    2. åˆç†çš„æ—¶é—´åˆ†é…
    3. å­¦ä¹ æ•ˆçŽ‡å»ºè®®
    4. å…·ä½“çš„æ‰§è¡Œæ­¥éª¤

    ç”¨ä¸­æ–‡å›žå¤ï¼Œæ ¼å¼æ¸…æ™°æ˜“è¯»ã€‚"""

        tasks_text = "\n".join([f"- {task}" for task in tasks])
        assignments_text = "\n".join([
            f"- {a.get('title', '')} (æˆªæ­¢: {a.get('due_date')}, çŠ¶æ€: {a.get('status', 'pending')})"
            for a in relevant_data['assignments'][:5]  # é™åˆ¶æ•°é‡é¿å…è¿‡é•¿
        ])

        user_prompt = f"""è¯·ä¸ºä»¥ä¸‹å­¦ä¹ ä»»åŠ¡ç”Ÿæˆæ—¥ç¨‹å®‰æŽ’ï¼š

    å¾…åŠžä»»åŠ¡ï¼š
    {tasks_text}

    çŽ°æœ‰ä½œä¸šï¼š
    {assignments_text}

    æ—¶é—´èŒƒå›´ï¼š{relevant_data['date_range']['start'].strftime('%Y-%m-%d')} åˆ° {relevant_data['date_range']['end'].strftime('%Y-%m-%d')}

    è¯·æä¾›ä¸€ä¸ªå®žç”¨çš„å­¦ä¹ è®¡åˆ’ï¼ŒåŒ…æ‹¬æ—¶é—´å®‰æŽ’å’Œä¼˜å…ˆçº§å»ºè®®ã€‚"""

        print("ðŸ§  è°ƒç”¨LLMç”Ÿæˆæ™ºèƒ½è§„åˆ’...")
        ai_response = self.call_llm(system_prompt, user_prompt)

        return {
            'ai_recommendation': ai_response,
            'tasks_analyzed': len(tasks),
            'assignments_considered': len(relevant_data['assignments'])
        }

    def _create_ai_schedule_response(self, ai_plan: Dict, relevant_data: Dict, tasks: List[str]) -> "ScheduleResult":
        """åˆ›å»ºAIå¢žå¼ºçš„æ—¥ç¨‹å“åº”"""

        schedule_items = []

        # 1. AIå»ºè®®
        schedule_items.append({
            'type': 'ai_recommendation',
            'content': ai_plan['ai_recommendation'],
            'source': 'llm',
            'tasks_analyzed': ai_plan['tasks_analyzed']
        })

        # 2. å…·ä½“ä»»åŠ¡å®‰æŽ’
        for i, task in enumerate(tasks):
            schedule_items.append({
                'type': 'scheduled_task',
                'task_id': i + 1,
                'title': task,
                'suggested_order': i + 1,
                'estimated_duration_minutes': 60,  # é»˜è®¤1å°æ—¶
                'priority': 'high' if i == 0 else 'medium'
            })

        # 3. æ•°æ®ç»Ÿè®¡
        schedule_items.append({
            'type': 'summary',
            'total_tasks': len(tasks),
            'upcoming_assignments': len(relevant_data['assignments']),
            'study_sessions': len(relevant_data['study_sessions']),
            'date_range': f"{relevant_data['date_range']['start'].strftime('%Y-%m-%d')} åˆ° {relevant_data['date_range']['end'].strftime('%Y-%m-%d')}"
        })

        return ScheduleResult(schedule=schedule_items)

    def _create_fallback_schedule(self, relevant_data: Dict, tasks: List[str]) -> "ScheduleResult":
        """LLMå¤±è´¥æ—¶çš„å›žé€€æ–¹æ¡ˆ"""
        schedule_items = []

        schedule_items.append({
            'type': 'info',
            'content': 'åŸºäºŽçŽ°æœ‰æ•°æ®çš„æ—¥ç¨‹å®‰æŽ’ï¼ˆAIè§„åˆ’æš‚ä¸å¯ç”¨ï¼‰'
        })

        # ç®€å•çš„ä»»åŠ¡å®‰æŽ’
        for i, task in enumerate(tasks):
            schedule_items.append({
                'type': 'task',
                'title': task,
                'order': i + 1,
                'suggested_time': f'ç¬¬{i + 1}å¤©'
            })

        return ScheduleResult(schedule=schedule_items)

    def _analyze_schedule_intent(self, query: str, date_range: str, user_id: int) -> Dict:
        """ä½¿ç”¨LLMåˆ†æžç”¨æˆ·æŸ¥è¯¢çš„æ·±å±‚æ„å›¾"""

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å­¦ä¹ è§„åˆ’åŠ©æ‰‹ã€‚è¯·åˆ†æžç”¨æˆ·çš„æ—¥ç¨‹æŸ¥è¯¢è¯·æ±‚ï¼Œç†è§£å…¶æ·±å±‚æ„å›¾å’Œéœ€æ±‚ã€‚

    è¯·åˆ†æžä»¥ä¸‹å†…å®¹ï¼š
    1. ä¸»è¦æ„å›¾ï¼ˆè€ƒè¯•å‡†å¤‡ã€ä½œä¸šè§„åˆ’ã€å­¦ä¹ å®‰æŽ’ã€å¤ä¹ è®¡åˆ’ç­‰ï¼‰
    2. æ—¶é—´ç´§è¿«ç¨‹åº¦
    3. ç”¨æˆ·å¯èƒ½çš„éšè—éœ€æ±‚
    4. æŽ¨èçš„è§„åˆ’ç­–ç•¥

    è¿”å›žJSONæ ¼å¼ï¼š{
        "intent": "è€ƒè¯•å‡†å¤‡|ä½œä¸šè§„åˆ’|å­¦ä¹ å®‰æŽ’|å¤ä¹ è®¡åˆ’",
        "urgency": "é«˜|ä¸­|ä½Ž",
        "time_range": {"start": "...", "end": "..."},
        "hidden_needs": ["..."],
        "planning_strategy": "...",
        "focus_courses": ["è¯¾ç¨‹å"],
        "priority_tasks": ["ä»»åŠ¡ç±»åž‹"]
    }"""

        user_prompt = f"""
    ç”¨æˆ·æŸ¥è¯¢: {query}
    æ—¶é—´èŒƒå›´: {date_range}
    ç”¨æˆ·ID: {user_id}

    å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}

    è¯·åˆ†æžè¿™ä¸ªæ—¥ç¨‹æŸ¥è¯¢è¯·æ±‚ã€‚
    """

        response = self.call_llm(system_prompt, user_prompt)

        try:
            return json.loads(response)
        except:
            # å¦‚æžœLLMè¿”å›žéžJSONï¼Œä½¿ç”¨è§„åˆ™å›žé€€
            return self._fallback_intent_analysis(query, date_range)

    def _fallback_intent_analysis(self, query: str, date_range: str) -> Dict:
        """Fallback method for intent analysis if LLM fails."""
        return {
            "intent": "general",
            "urgency": "medium",
            "time_range": {"start": date_range.split(' to ')[0], "end": date_range.split(' to ')[1]},
            "hidden_needs": [],
            "planning_strategy": "rule_based",
            "focus_courses": [],
            "priority_tasks": []
        }

    def _calculate_time_range_based_on_urgency(self, urgency: str) -> int:
        """Calculate the number of days to look ahead based on urgency."""
        if urgency == "high":
            return 7
        elif urgency == "medium":
            return 14
        else:
            return 30

    def _get_exam_preparation_sessions(self, user_id: int, focus_courses: List[str]) -> List[Dict]:
        """Retrieve study sessions focused on exam preparation."""
        return self.study_session_dao.get_sessions_by_courses(user_id, focus_courses) if hasattr(self.study_session_dao, 'get_sessions_by_courses') else []

    def _analyze_study_patterns(self, study_sessions: List[Dict]) -> str:
        """Analyze study patterns from past sessions."""
        if not study_sessions:
            return "No recent study patterns available."
        total_hours = sum(session.get("duration", 0) for session in study_sessions)
        return f"Total study hours: {total_hours}"

    def _get_user_learning_profile(self, user_id: int) -> str:
        """Retrieve the user's learning profile."""
        return self.user_course_dao.get_learning_profile(user_id) if hasattr(self.user_course_dao, 'get_learning_profile') else "Default Profile"

    def _calculate_confidence(self, intent_analysis: Dict, relevant_data: Dict) -> float:
        """Calculate confidence score for the generated plan."""
        return 0.9 if intent_analysis.get("intent") == "exam_preparation" else 0.7

    def _extract_structured_plan(self, ai_recommendations: str) -> List[Dict]:
        """Extract structured plan items from AI recommendations."""
        # Placeholder logic for parsing AI recommendations
        return [{"task": "Example Task", "time": "10:00 AM", "priority": "high", "duration": "1 hour", "reasoning": "Example reasoning."}]

    def _calculate_recent_study_hours(self, study_sessions: List[Dict]) -> int:
        """Calculate total recent study hours."""
        return sum(session.get("duration", 0) for session in study_sessions)

    def action(self, action_input: ActionInput) -> ActionResult:
        """
        Handle user actions dynamically, such as marking assignments complete,
        querying pending tasks, or generating study plans.
        """
        try:
            action = action_input.action
            data = action_input.data

            if action == "mark_assignment_complete":
                assignment_id = data.get("assignment_id")
                user_id = data.get("user_id")
                if not assignment_id or not user_id:
                    return ActionResult(status="error", message="Missing assignment_id or user_id")

                success = self.assignment_dao.mark_complete(assignment_id, user_id)
                if success:
                    return ActionResult(status="success", message="Assignment marked as complete")
                else:
                    return ActionResult(status="error", message="Failed to mark assignment as complete")

            elif action == "query_pending_assignments":
                user_id = data.get("user_id")
                if not user_id:
                    return ActionResult(status="error", message="Missing user_id")

                pending_assignments = self.assignment_dao.get_pending_by_user(user_id) if hasattr(self.assignment_dao, 'get_pending_by_user') else []
                return ActionResult(status="success", data={"pending_assignments": pending_assignments})

            elif action == "generate_study_plan":
                schedule_input = ScheduleInput(**data)
                schedule_result = self.schedule(schedule_input)
                return ActionResult(status="success", data={"schedule": schedule_result.schedule})

            else:
                return ActionResult(status="error", message=f"Unknown action: {action}")

        except Exception as e:
            return ActionResult(status="error", message=f"Action failed: {str(e)}")

    def schedule(self, scheduleInput: ScheduleInput) -> ScheduleResult:
        """
        Generate a study schedule based on assignments and study sessions.
        """
        try:
            user_id = scheduleInput.user_id
            tasks = scheduleInput.tasks
            date_range = scheduleInput.date_range

            if not user_id:
                return ScheduleResult(schedule=[{"type": "error", "content": "User ID is required"}])

            # Fetch relevant data
            relevant_data = self._get_relevant_data(user_id, date_range)

            # Generate intelligent plan using LLM
            try:
                ai_plan = self._generate_intelligent_plan(tasks, relevant_data, user_id)
                return self._create_ai_schedule_response(ai_plan, relevant_data, tasks)
            except Exception as e:
                print(f"LLM planning failed, falling back to rule-based scheduling: {e}")
                return self._create_rule_based_schedule(relevant_data, tasks)

        except Exception as e:
            print(f"Schedule generation error: {e}")
            return ScheduleResult(schedule=[{"type": "error", "content": f"Failed to generate schedule: {str(e)}"}])

    def _get_relevant_data(self, user_id: int, date_range: str) -> Dict:
        """
        Fetch assignments and study sessions within the specified date range.
        """
        from datetime import datetime, timedelta

        # Parse date range
        if date_range and " to " in date_range:
            start_str, end_str = date_range.split(" to ")
            start_date = datetime.strptime(start_str.strip(), "%Y-%m-%d")
            end_date = datetime.strptime(end_str.strip(), "%Y-%m-%d")
        else:
            start_date = datetime.now()
            end_date = start_date + timedelta(days=30)

        assignments = self.assignment_dao.get_assignments_by_date_range(user_id, start_date, end_date)
        study_sessions = self.study_session_dao.get_study_sessions_by_date_range(user_id, start_date, end_date)

        return {
            "assignments": assignments,
            "study_sessions": study_sessions,
            "date_range": {"start": start_date, "end": end_date}
        }

    def _create_rule_based_schedule(self, relevant_data: Dict, tasks: List[str]) -> "ScheduleResult":
        """Fallback rule-based schedule creation."""
        schedule_items = []
        for i, task in enumerate(tasks):
            schedule_items.append({
                'type': 'task',
                'title': task,
                'order': i + 1,
                'suggested_time': f'Day {i + 1}'
            })
        return ScheduleResult(schedule=schedule_items)
